
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../_static/javascripts/modernizr.js"></script>
  
  <script>
      window.ga = window.ga || function () {
          (ga.q = ga.q || []).push(arguments)
      };
      ga.l = +new Date;
      ga('create', 'G-F9LD9HL8LW', 'auto');
      ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  
  
    <title>C++ Examples &#8212; Kompute Documentation (Python &amp; C++)</title>
    <link rel="stylesheet" href="../_static/material.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/assets/custom.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Memory Management Principles" href="memory-management.html" />
    <link rel="prev" title="Community Meetings, Channels &amp; Resources" href="community.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=red data-md-color-accent=light-blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#overview/advanced-examples" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../index.html" title="Kompute Documentation (Python &amp; C++)"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Kompute Documentation (Python & C++)</span>
          <span class="md-header-nav__topic"> C++ Examples </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../search.html" method="GET" name="search">
      <input type="text" class="md-search__input" name="q" placeholder="Search"
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            <a href="https://github.com/KomputeProject/kompute/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Kompute
  </div>
</a>
          </div>
        </div>
      
      
  
  <script src="../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../"versions.json"",
        target_loc = "../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../index.html" class="md-tabs__link">Kompute Documentation (Python & C++)</a></li>
            
            <li class="md-tabs__item"><a href="https://github.com/KomputeProject/kompute/" class="md-tabs__link">Kompute Repo</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../index.html" title="Kompute Documentation (Python &amp; C++)" class="md-nav__button md-logo">
      
        <img src="../_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../index.html"
       title="Kompute Documentation (Python &amp; C++)">Kompute Documentation (Python & C++)</a>
  </label>
    <div class="md-nav__source">
      <a href="https://github.com/KomputeProject/kompute/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Kompute
  </div>
</a>
    </div>
  
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">C++ Documentation:</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="https://kompute.cc/" class="md-nav__link">Documentation Home</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="community.html" class="md-nav__link">Community Meetings, Discord & Links</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">C++ Documentation:</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    <label class="md-nav__link md-nav__link--active" for="__toc"> C++ Examples </label>
    
      <a href="#" class="md-nav__link md-nav__link--active">C++ Examples</a>
      
        
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">Contents</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#overview-advanced-examples--page-root" class="md-nav__link">C++ Examples</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#simple-examples" class="md-nav__link">Simple examples</a>
        </li>
        <li class="md-nav__item"><a href="#end-to-end-examples" class="md-nav__link">End-to-end examples</a>
        </li>
        <li class="md-nav__item"><a href="#add-extensions" class="md-nav__link">Add Extensions</a>
        </li>
        <li class="md-nav__item"><a href="#your-custom-kompute-operation" class="md-nav__link">Your Custom Kompute Operation</a>
        </li>
        <li class="md-nav__item"><a href="#async-await-example" class="md-nav__link">Async/Await Example</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#parallel-operation-submission" class="md-nav__link">Parallel Operation Submission</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#conceptual-overview" class="md-nav__link">Conceptual Overview</a>
        </li>
        <li class="md-nav__item"><a href="#parallel-execution-example" class="md-nav__link">Parallel Execution Example</a>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/overview/advanced-examples.rst.txt">Show Source</a> </li>

  </ul>
</nav>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="memory-management.html" class="md-nav__link">Memory Management Principles</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="build-system.html" class="md-nav__link">Build System Deep Dive</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="shaders-to-headers.html" class="md-nav__link">Processing Shaders (Online & Offline)</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="custom-operations.html" class="md-nav__link">Extending Kompute with Custom Operations</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="reference.html" class="md-nav__link">C++ Class Documentation & Reference</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="https://kompute.cc/codecov/" class="md-nav__link">Test Code Coverage</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Python Documentation:</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="python-package.html" class="md-nav__link">Python Package Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="python-reference.html" class="md-nav__link">Python Class Documentation & Reference</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Examples:</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="python-examples.html" class="md-nav__link">Python Examples</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    <label class="md-nav__link md-nav__link--active" for="__toc"> C++ Examples </label>
    
      <a href="#" class="md-nav__link md-nav__link--active">C++ Examples</a>
      
        
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">Contents</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#overview-advanced-examples--page-root" class="md-nav__link">C++ Examples</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#simple-examples" class="md-nav__link">Simple examples</a>
        </li>
        <li class="md-nav__item"><a href="#end-to-end-examples" class="md-nav__link">End-to-end examples</a>
        </li>
        <li class="md-nav__item"><a href="#add-extensions" class="md-nav__link">Add Extensions</a>
        </li>
        <li class="md-nav__item"><a href="#your-custom-kompute-operation" class="md-nav__link">Your Custom Kompute Operation</a>
        </li>
        <li class="md-nav__item"><a href="#async-await-example" class="md-nav__link">Async/Await Example</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#parallel-operation-submission" class="md-nav__link">Parallel Operation Submission</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#conceptual-overview" class="md-nav__link">Conceptual Overview</a>
        </li>
        <li class="md-nav__item"><a href="#parallel-execution-example" class="md-nav__link">Parallel Execution Example</a>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/overview/advanced-examples.rst.txt">Show Source</a> </li>

  </ul>
</nav>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="mobile-android.html" class="md-nav__link">Android Mobile App Integration</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="raspberry-pi.html" class="md-nav__link">Edge Device Raspberry Pi Mesa Drivers</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="game-engine-godot.html" class="md-nav__link">Game Engine Godot Integration</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="matmul-benchmark.html" class="md-nav__link">Example Benchmark with Matrix Multiplication</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="convolutional-net.html" class="md-nav__link">Convolutional Neural Network (CNN) Simple Upscale</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Advanced Concepts &amp; Deep Dives:</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="ci-tests.html" class="md-nav__link">CI, Docker Images Docs & Tests</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="variable-types.html" class="md-nav__link">Variable Types for Tensors, and Push/Spec Constants</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="async-parallel.html" class="md-nav__link">Asynchronous & Parallel Operations</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../genindex.html" class="md-nav__link">Code Index</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">Contents</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#overview-advanced-examples--page-root" class="md-nav__link">C++ Examples</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#simple-examples" class="md-nav__link">Simple examples</a>
        </li>
        <li class="md-nav__item"><a href="#end-to-end-examples" class="md-nav__link">End-to-end examples</a>
        </li>
        <li class="md-nav__item"><a href="#add-extensions" class="md-nav__link">Add Extensions</a>
        </li>
        <li class="md-nav__item"><a href="#your-custom-kompute-operation" class="md-nav__link">Your Custom Kompute Operation</a>
        </li>
        <li class="md-nav__item"><a href="#async-await-example" class="md-nav__link">Async/Await Example</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#parallel-operation-submission" class="md-nav__link">Parallel Operation Submission</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#conceptual-overview" class="md-nav__link">Conceptual Overview</a>
        </li>
        <li class="md-nav__item"><a href="#parallel-execution-example" class="md-nav__link">Parallel Execution Example</a>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/overview/advanced-examples.rst.txt">Show Source</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="c-examples">
<h1 id="overview-advanced-examples--page-root">C++ Examples<a class="headerlink" href="#overview-advanced-examples--page-root" title="Permalink to this headline">¶</a></h1>
<p>The power of Kompute comes in when the interface is used for complex computations. This section contains an outline of the advanced / end-to-end examples available.</p>
<section id="simple-examples">
<h2 id="simple-examples">Simple examples<a class="headerlink" href="#simple-examples" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#your-custom-kompute-operation">Create your custom Kompute Operations</a></p></li>
<li><p><a class="reference external" href="#asynchronous-operations">Run Asynchronous Operations</a></p></li>
<li><p><a class="reference external" href="#parallel-operations">Run Parallel Operations Across Multiple GPU Queues</a></p></li>
</ul>
</section>
<section id="end-to-end-examples">
<h2 id="end-to-end-examples">End-to-end examples<a class="headerlink" href="#end-to-end-examples" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a">Machine Learning Logistic Regression Implementation</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/parallelizing-heavy-gpu-workloads-via-multi-queue-operations-50a38b15a1dc">Parallelizing GPU-intensive Workloads via Multi-Queue Operations</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/gpu-accelerated-machine-learning-in-your-mobile-applications-using-the-android-ndk-vulkan-kompute-1e9da37b7617">Android NDK Mobile Kompute ML Application</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/supercharging-game-development-with-gpu-accelerated-ml-using-vulkan-kompute-the-godot-game-engine-4e75a84ea9f0">Game Development Kompute ML in Godot Engine</a></p></li>
</ul>
</section>
<section id="add-extensions">
<h2 id="add-extensions">Add Extensions<a class="headerlink" href="#add-extensions" title="Permalink to this headline">¶</a></h2>
<p>Kompute provides a simple way to add extensions through kp::Manager initialisation. When debug is enabled you will be able to see logs that show what are the desired extensions requested and the ones that are added based on the available extensions on the current driver.</p>
<p>The example below shows how you can enable the “VK_EXT_shader_atomic_float” extension so we can use the adomicAdd for floats in the shaders.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span></pre></div></td><td class="code"><div><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">shader</span><span class="p">(</span><span class="sa">R</span><span class="s">"</span><span class="dl">(</span>
<span class="s">          #version 450</span>

<span class="s">          #extension GL_EXT_shader_atomic_float: enable</span>

<span class="s">          layout(push_constant) uniform PushConstants {</span>
<span class="s">            float x;</span>
<span class="s">            float y;</span>
<span class="s">            float z;</span>
<span class="s">          } pcs;</span>

<span class="s">          layout (local_size_x = 1) in;</span>

<span class="s">          layout(set = 0, binding = 0) buffer a { float pa[]; };</span>

<span class="s">          void main() {</span>
<span class="s">              atomicAdd(pa[0], pcs.x);</span>
<span class="s">              atomicAdd(pa[1], pcs.y);</span>
<span class="s">              atomicAdd(pa[2], pcs.z);</span>
<span class="s">          }</span><span class="dl">)</span><span class="s">"</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// See shader documentation section for compileSource</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">spirv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compileSource</span><span class="p">(</span><span class="n">shader</span><span class="p">);</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Sequence</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>

<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="n">kp</span><span class="o">::</span><span class="n">Manager</span><span class="w"> </span><span class="n">mgr</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="p">{},</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="s">"VK_EXT_shader_atomic_float"</span><span class="w"> </span><span class="p">});</span>

<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">tensor</span><span class="p">({</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">});</span>

<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Algorithm</span><span class="o">&gt;</span><span class="w"> </span><span class="n">algo</span><span class="w"> </span><span class="o">=</span>
<span class="w">          </span><span class="n">mgr</span><span class="p">.</span><span class="n">algorithm</span><span class="p">({</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="n">spirv</span><span class="p">,</span><span class="w"> </span><span class="n">kp</span><span class="o">::</span><span class="n">Workgroup</span><span class="p">({</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">}),</span><span class="w"> </span><span class="p">{},</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="w"> </span><span class="p">});</span>

<span class="w">        </span><span class="n">sq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">sequence</span><span class="p">()</span>
<span class="w">               </span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncDevice</span><span class="o">&gt;</span><span class="p">({</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="p">})</span>
<span class="w">               </span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpAlgoDispatch</span><span class="o">&gt;</span><span class="p">(</span><span class="n">algo</span><span class="p">,</span>
<span class="w">                                            </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">{</span><span class="w"> </span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w"> </span><span class="mf">0.3</span><span class="w"> </span><span class="p">})</span>
<span class="w">               </span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpAlgoDispatch</span><span class="o">&gt;</span><span class="p">(</span><span class="n">algo</span><span class="p">,</span>
<span class="w">                                            </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">{</span><span class="w"> </span><span class="mf">0.3</span><span class="p">,</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1</span><span class="w"> </span><span class="p">})</span>
<span class="w">               </span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncLocal</span><span class="o">&gt;</span><span class="p">({</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="p">})</span>
<span class="w">               </span><span class="o">-&gt;</span><span class="n">eval</span><span class="p">();</span>

<span class="w">        </span><span class="n">EXPECT_EQ</span><span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">({</span><span class="w"> </span><span class="mf">0.4</span><span class="p">,</span><span class="w"> </span><span class="mf">0.4</span><span class="p">,</span><span class="w"> </span><span class="mf">0.4</span><span class="w"> </span><span class="p">}));</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div></td></tr></table></div>
</div>
</section>
<section id="your-custom-kompute-operation">
<h2 id="your-custom-kompute-operation">Your Custom Kompute Operation<a class="headerlink" href="#your-custom-kompute-operation" title="Permalink to this headline">¶</a></h2>
<p>Build your own pre-compiled operations for domain specific workflows. Back to <a class="reference external" href="#simple-examples">examples list</a></p>
<p>We also provide tools that allow you to <a class="reference external" href="https://github.com/KomputeProject/kompute/blob/master/scripts/convert_shaders.py#L40">convert shaders into C++ headers</a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span></pre></div></td><td class="code"><div><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">OpMyCustom</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">OpAlgoDispatch</span>
<span class="p">{</span>
<span class="w">  </span><span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">OpMyCustom</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">,</span>
<span class="w">         </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Algorithm</span><span class="o">&gt;</span><span class="w"> </span><span class="n">algorithm</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">OpAlgoBase</span><span class="p">(</span><span class="n">algorithm</span><span class="p">)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">         </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensors</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">             </span><span class="k">throw</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">"Kompute OpMult expected 3 tensors but got "</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">         </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// See shader documentation section for compileSource</span>
<span class="w">         </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">spirv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compileSource</span><span class="p">(</span><span class="sa">R</span><span class="s">"</span><span class="dl">(</span>
<span class="s">             #version 450</span>

<span class="s">             layout(set = 0, binding = 0) buffer tensorLhs {</span>
<span class="s">                float valuesLhs[ ];</span>
<span class="s">             };</span>

<span class="s">             layout(set = 0, binding = 1) buffer tensorRhs {</span>
<span class="s">                float valuesRhs[ ];</span>
<span class="s">             };</span>

<span class="s">             layout(set = 0, binding = 2) buffer tensorOutput {</span>
<span class="s">                float valuesOutput[ ];</span>
<span class="s">             };</span>

<span class="s">             layout (constant_id = 0) const uint LEN_LHS = 0;</span>
<span class="s">             layout (constant_id = 1) const uint LEN_RHS = 0;</span>
<span class="s">             layout (constant_id = 2) const uint LEN_OUT = 0;</span>

<span class="s">             layout (local_size_x = 1, local_size_y = 1, local_size_z = 1) in;</span>

<span class="s">             void main()</span>
<span class="s">             {</span>
<span class="s">                 uint index = gl_GlobalInvocationID.x;</span>

<span class="s">                 valuesOutput[index] = valuesLhs[index] * valuesRhs[index];</span>
<span class="s">             }</span>
<span class="s">         </span><span class="dl">)</span><span class="s">"</span><span class="p">);</span>

<span class="w">         </span><span class="n">algorithm</span><span class="o">-&gt;</span><span class="n">rebuild</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">spirv</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="n">kp</span><span class="o">::</span><span class="n">Manager</span><span class="w"> </span><span class="n">mgr</span><span class="p">;</span><span class="w"> </span><span class="c1">// Automatically selects Device 0</span>

<span class="w">    </span><span class="c1">// Create 3 tensors of default type float</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tensorLhs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">tensor</span><span class="p">({</span><span class="w"> </span><span class="mf">0.</span><span class="p">,</span><span class="w"> </span><span class="mf">1.</span><span class="p">,</span><span class="w"> </span><span class="mf">2.</span><span class="w"> </span><span class="p">});</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tensorRhs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">tensor</span><span class="p">({</span><span class="w"> </span><span class="mf">2.</span><span class="p">,</span><span class="w"> </span><span class="mf">4.</span><span class="p">,</span><span class="w"> </span><span class="mf">6.</span><span class="w"> </span><span class="p">});</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tensorOut</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">tensor</span><span class="p">({</span><span class="w"> </span><span class="mf">0.</span><span class="p">,</span><span class="w"> </span><span class="mf">0.</span><span class="p">,</span><span class="w"> </span><span class="mf">0.</span><span class="w"> </span><span class="p">});</span>

<span class="w">    </span><span class="n">mgr</span><span class="p">.</span><span class="n">sequence</span><span class="p">()</span>
<span class="w">         </span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncDevice</span><span class="o">&gt;</span><span class="p">({</span><span class="n">tensorLhs</span><span class="p">,</span><span class="w"> </span><span class="n">tensorRhs</span><span class="p">,</span><span class="w"> </span><span class="n">tensorOut</span><span class="p">})</span>
<span class="w">         </span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpMyCustom</span><span class="o">&gt;</span><span class="p">({</span><span class="n">tensorLhs</span><span class="p">,</span><span class="w"> </span><span class="n">tensorRhs</span><span class="p">,</span><span class="w"> </span><span class="n">tensorOut</span><span class="p">},</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">algorithm</span><span class="p">())</span>
<span class="w">         </span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncLocal</span><span class="o">&gt;</span><span class="p">({</span><span class="n">tensorLhs</span><span class="p">,</span><span class="w"> </span><span class="n">tensorRhs</span><span class="p">,</span><span class="w"> </span><span class="n">tensorOut</span><span class="p">})</span>
<span class="w">         </span><span class="o">-&gt;</span><span class="n">eval</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// Prints the output which is { 0, 4, 12 }</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(</span><span class="s">"Output: {}"</span><span class="p">,</span><span class="w"> </span><span class="n">tensorOutput</span><span class="p">.</span><span class="n">data</span><span class="p">())</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div></td></tr></table></div>
</div>
</section>
<section id="async-await-example">
<h2 id="async-await-example">Async/Await Example<a class="headerlink" href="#async-await-example" title="Permalink to this headline">¶</a></h2>
<p>A simple example of asynchronous submission can be found below.</p>
<p>First we are able to create the manager as we normally would.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div><pre><span></span><span class="c1">// You can allow Kompute to create the GPU resources, or pass your existing ones</span>
<span class="n">kp</span><span class="o">::</span><span class="n">Manager</span><span class="w"> </span><span class="n">mgr</span><span class="p">;</span><span class="w"> </span><span class="c1">// Selects device 0 unless explicitly requested</span>

<span class="c1">// Creates tensor an initializes GPU memory (below we show more granularity)</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">);</span>
</pre></div></td></tr></table></div>
</div>
<p>We can now run our first asynchronous command, which in this case we can use the default sequence.</p>
<p>Sequences can be executed in synchronously or asynchronously without having to change anything.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><span class="c1">// Create tensors data explicitly in GPU with an operation</span>
<span class="n">mgr</span><span class="p">.</span><span class="n">sequence</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">eval</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncDevice</span><span class="o">&gt;</span><span class="p">({</span><span class="n">tensor</span><span class="p">});</span>
</pre></div></td></tr></table></div>
</div>
<p>While this is running we can actually do other things like in this case create the shader we’ll be using.</p>
<p>In this case we create a shader that should take a couple of milliseconds to run.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span></pre></div></td><td class="code"><div><pre><span></span><span class="c1">// Define your shader as a string (using string literals for simplicity)</span>
<span class="c1">// (You can also pass the raw compiled bytes, or even path to file)</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">shader</span><span class="p">(</span><span class="sa">R</span><span class="s">"</span><span class="dl">(</span>
<span class="s">    #version 450</span>

<span class="s">    layout (local_size_x = 1) in;</span>

<span class="s">    layout(set = 0, binding = 0) buffer b { float pb[]; };</span>

<span class="s">    shared uint sharedTotal[1];</span>

<span class="s">    void main() {</span>
<span class="s">        uint index = gl_GlobalInvocationID.x;</span>

<span class="s">        sharedTotal[0] = 0;</span>

<span class="s">        // Iterating to simulate longer process</span>
<span class="s">        for (int i = 0; i &lt; 100000000; i++)</span>
<span class="s">        {</span>
<span class="s">            atomicAdd(sharedTotal[0], 1);</span>
<span class="s">        }</span>

<span class="s">        pb[index] = sharedTotal[0];</span>
<span class="s">    }</span>
<span class="dl">)</span><span class="s">"</span><span class="p">);</span>

<span class="c1">// See shader documentation section for compileSource</span>
<span class="k">auto</span><span class="w"> </span><span class="n">algo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">algorithm</span><span class="p">({</span><span class="n">tensor</span><span class="p">},</span><span class="w"> </span><span class="n">compileSource</span><span class="p">(</span><span class="n">shader</span><span class="p">));</span>
</pre></div></td></tr></table></div>
</div>
<p>Now we are able to run the await function on the default sequence.</p>
<p>If we are using the manager, we need to make sure that we are awaiting the same named sequence that was triggered asynchronously.</p>
<p>If the sequence is not running or has finished running, it would return immediately.</p>
<p>The parameter provided is the maximum amount of time to wait in nanoseconds. When the timeout expires, the sequence would return (with false value), but it does not stop the processing in the GPU - the processing would continue as normal.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span></pre></div></td><td class="code"><div><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">sq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">sequence</span><span class="p">()</span>

<span class="c1">// Run Async Kompute operation on the parameters provided</span>
<span class="n">sq</span><span class="o">-&gt;</span><span class="n">evalAsync</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpAlgoDispatch</span><span class="o">&gt;</span><span class="p">(</span><span class="n">algo</span><span class="p">);</span>

<span class="c1">// Here we can do other work</span>

<span class="c1">// When we're ready we can wait</span>
<span class="c1">// The default wait time is UINT64_MAX</span>
<span class="n">sq</span><span class="p">.</span><span class="n">evalAwait</span><span class="p">()</span>
</pre></div></td></tr></table></div>
</div>
<p>Finally, below you can see that we can also run syncrhonous commands without having to change anything.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span></pre></div></td><td class="code"><div><pre><span></span><span class="c1">// Sync the GPU memory back to the local tensor</span>
<span class="c1">// We can still run synchronous jobs in our created sequence</span>
<span class="n">sq</span><span class="p">.</span><span class="n">eval</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncLocal</span><span class="o">&gt;</span><span class="p">({</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="p">});</span>

<span class="c1">// Prints the output: B: { 100000000, ... }</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(</span><span class="s">"B: {}"</span><span class="p">,</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">data</span><span class="p">())</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div></td></tr></table></div>
</div>
<section id="parallel-operation-submission">
<h3 id="parallel-operation-submission">Parallel Operation Submission<a class="headerlink" href="#parallel-operation-submission" title="Permalink to this headline">¶</a></h3>
<p>In order to work with parallel execution of tasks, it is important that you understand some of the core GPU processing limitations, as these can be quite broad and hardware dependent, which means they will vary across NVIDIA / AMD / ETC video cards.</p>
</section>
</section>
<section id="conceptual-overview">
<h2 id="conceptual-overview">Conceptual Overview<a class="headerlink" href="#conceptual-overview" title="Permalink to this headline">¶</a></h2>
<p>If you are familiar with the Vulkan SDK, you will have experience that the first few things you do is fetching the physical Queues from the device. The queues themselves tend to have three main particular features - they can be GRAPHICS, TRANSFER and COMPUTE (between a few others we’ll skip for simplicity).</p>
<p>Queues can have multiple properties - namely a queue can be of type GRAPHICS+TRANSFER+COMPUTE, etc. Now here comes the key point: the underlying hardware may (or may not) support parallelized processing at multiple levels.</p>
<p>Let’s take a tangible example. The [NVIDIA 1650](<a class="reference external" href="http://vulkan.gpuinfo.org/displayreport.php?id=9700#queuefamilies">http://vulkan.gpuinfo.org/displayreport.php?id=9700#queuefamilies</a>) for example has 16 <cite>GRAPHICS+TRANSFER+COMPUTE</cite> queues on <cite>familyIndex 0</cite>, then 2 <cite>TRANSFER</cite> queues in <cite>familyIndex 1</cite> and finally 8 <cite>COMPUTE+TRANSFER</cite> queues in <cite>familyIndex 2</cite>.</p>
<p>With this in mind, the NVIDIA 1650 as of today does not support intra-family parallelization, which means that if you were to submit commands in multiple queues of the same family, these would still be exectured synchronously.</p>
<p>However the NVIDIA 1650 does support inter-family parallelization, which means that if we were to submit commands across multiple queues from different families, these would execute in parallel.</p>
<p>This means that we would be able to execute parallel workloads as long as we’re running them across multiple queue families. This is one of the reasons why Kompute enables users to explicitly select the underlying queues and queue families to run particular workloads on.</p>
<p>It is important that you understand what are the capabilities and limitations of your hardware, as parallelization capabilities can vary, so you will want to make sure you account for potential discrepancies in processing structures, mainyl to avoid undesired/unexpected race conditions.</p>
</section>
<section id="parallel-execution-example">
<h2 id="parallel-execution-example">Parallel Execution Example<a class="headerlink" href="#parallel-execution-example" title="Permalink to this headline">¶</a></h2>
<p>In this example we will demonstrate how you can set up parallel processing across two compute families to achieve 2x speedups when running processing workloads.</p>
<p>To start, you will see that we do have to create the manager with extra parameters. This includes the GPU device index we want to use, together with the array of the queues that we want to enable.</p>
<p>In this case we are using only two queues, which as per the section above, these would be familyIndex 0 which is of type <cite>GRAPHICS+COMPUTE+TRANSFER</cite> and familyIndex 2 which is of type <cite>COMPUTE+TRANSFER</cite>.</p>
<p>In this case based on the specifications of the NVIDIA 1650 we could define up to 16 graphics queues (familyIndex 0), 2 transfer queues (familyIndex 1), and 8 compute queues (familyIndex 2) in no particular order. This means that we could have something like <cite>{ 0, 1, 1, 2, 2, 2, 0, … }</cite> as our initialization value.</p>
<p>You will want to keep track of the indices you initialize your manager, as you will be referring back to this ordering when creating sequences with particular queues.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span></pre></div></td><td class="code"><div><pre><span></span><span class="w">   </span><span class="c1">// In this case we select device 0, and for queues, one queue from familyIndex 0</span>
<span class="w">   </span><span class="c1">// and one queue from familyIndex 2</span>
<span class="w">   </span><span class="kt">uint32_t</span><span class="w"> </span><span class="nf">deviceIndex</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">familyIndices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">};</span>

<span class="w">   </span><span class="c1">// We create a manager with device index, and queues by queue family index</span>
<span class="w">   </span><span class="n">kp</span><span class="o">::</span><span class="n">Manager</span><span class="w"> </span><span class="nf">mgr</span><span class="p">(</span><span class="n">deviceIndex</span><span class="p">,</span><span class="w"> </span><span class="n">familyIndices</span><span class="p">);</span>
</pre></div></td></tr></table></div>
</div>
<p>We are now able to create sequences with a particular queue.</p>
<p>By default the Kompute Manager is created with device 0, and with a single queue of the first compatible familyIndex. Similarly, by default sequences are created with the first available queue.</p>
<p>In this case we are able to specify which queue we want to use. Below we initialize “queueOne” named sequence with the graphics family queue, and “queueTwo” with the compute family queue.</p>
<p>It’s worth mentioning you can have multiple sequences referencing the same queue.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div><pre><span></span><span class="w">   </span><span class="c1">// We need to create explicit sequences with their respective queues</span>
<span class="w">   </span><span class="c1">// The second parameter is the index in the familyIndex array which is relative</span>
<span class="w">   </span><span class="c1">//      to the vector we created the manager with.</span>
<span class="w">   </span><span class="n">sqOne</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">sequence</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">   </span><span class="n">sqTwo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">sequence</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</pre></div></td></tr></table></div>
</div>
<p>We create the tensors without modifications.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div><pre><span></span><span class="w">   </span><span class="c1">// Creates tensor an initializes GPU memory (below we show more granularity)</span>
<span class="w">   </span><span class="k">auto</span><span class="w"> </span><span class="n">tensorA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">tensor</span><span class="p">({</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="w"> </span><span class="p">});</span>
<span class="w">   </span><span class="k">auto</span><span class="w"> </span><span class="n">tensorB</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">tensor</span><span class="p">({</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="w"> </span><span class="p">});</span>

<span class="w">   </span><span class="c1">// Copies the data into GPU memory</span>
<span class="w">   </span><span class="n">mgr</span><span class="p">.</span><span class="n">sequence</span><span class="p">().</span><span class="n">eval</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncDevice</span><span class="o">&gt;</span><span class="p">({</span><span class="n">tensorA</span><span class="w"> </span><span class="n">tensorB</span><span class="p">});</span>
</pre></div></td></tr></table></div>
</div>
<p>Similar to the asyncrhonous usecase above, we can still run synchronous commands without modifications.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span></pre></div></td><td class="code"><div><pre><span></span><span class="w">   </span><span class="c1">// Define your shader as a string (using string literals for simplicity)</span>
<span class="w">   </span><span class="c1">// (You can also pass the raw compiled bytes, or even path to file)</span>
<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">shader</span><span class="p">(</span><span class="sa">R</span><span class="s">"</span><span class="dl">(</span>
<span class="s">       #version 450</span>

<span class="s">       layout (local_size_x = 1) in;</span>

<span class="s">       layout(set = 0, binding = 0) buffer b { float pb[]; };</span>

<span class="s">       shared uint sharedTotal[1];</span>

<span class="s">       void main() {</span>
<span class="s">           uint index = gl_GlobalInvocationID.x;</span>

<span class="s">           sharedTotal[0] = 0;</span>

<span class="s">           // Iterating to simulate longer process</span>
<span class="s">           for (int i = 0; i &lt; 100000000; i++)</span>
<span class="s">           {</span>
<span class="s">               atomicAdd(sharedTotal[0], 1);</span>
<span class="s">           }</span>

<span class="s">           pb[index] = sharedTotal[0];</span>
<span class="s">       }</span>
<span class="s">   </span><span class="dl">)</span><span class="s">"</span><span class="p">);</span>

<span class="w">   </span><span class="c1">// See shader documentation section for compileSource</span>
<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">spirv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compileSource</span><span class="p">(</span><span class="n">shader</span><span class="p">);</span>

<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Algorithm</span><span class="o">&gt;</span><span class="w"> </span><span class="n">algo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mgr</span><span class="p">.</span><span class="n">algorithm</span><span class="p">({</span><span class="n">tensorA</span><span class="p">,</span><span class="w"> </span><span class="n">tenssorB</span><span class="p">},</span><span class="w"> </span><span class="n">spirv</span><span class="p">);</span>
</pre></div></td></tr></table></div>
</div>
<p>Now we can actually trigger the parallel processing, running two OpAlgoBase Operations - each in a different sequence / queue.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div><pre><span></span><span class="w">   </span><span class="c1">// Run the first parallel operation in the `queueOne` sequence</span>
<span class="w">   </span><span class="n">sqOne</span><span class="o">-&gt;</span><span class="n">evalAsync</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpAlgoDispatch</span><span class="o">&gt;</span><span class="p">(</span><span class="n">algo</span><span class="p">);</span>

<span class="w">   </span><span class="c1">// Run the second parallel operation in the `queueTwo` sequence</span>
<span class="w">   </span><span class="n">sqTwo</span><span class="o">-&gt;</span><span class="n">evalAsync</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpAlgoDispatch</span><span class="o">&gt;</span><span class="p">(</span><span class="n">algo</span><span class="p">);</span>
</pre></div></td></tr></table></div>
</div>
<p>Similar to the asynchronous example above, we are able to do other work whilst the tasks are executing.</p>
<p>We are able to wait for the tasks to complete by triggering the <cite>evalOpAwait</cite> on the respective sequence.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span></pre></div></td><td class="code"><div><pre><span></span><span class="w">   </span><span class="c1">// Here we can do other work</span>

<span class="w">   </span><span class="c1">// We can now wait for the two parallel tasks to finish</span>
<span class="w">   </span><span class="n">sqOne</span><span class="p">.</span><span class="n">evalOpAwait</span><span class="p">()</span>
<span class="w">   </span><span class="n">sqTwo</span><span class="p">.</span><span class="n">evalOpAwait</span><span class="p">()</span>

<span class="w">   </span><span class="c1">// Sync the GPU memory back to the local tensor</span>
<span class="w">   </span><span class="n">mgr</span><span class="p">.</span><span class="n">sequence</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">eval</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncLocal</span><span class="o">&gt;</span><span class="p">({</span><span class="w"> </span><span class="n">tensorA</span><span class="p">,</span><span class="w"> </span><span class="n">tensorB</span><span class="w"> </span><span class="p">});</span>

<span class="w">   </span><span class="c1">// Prints the output: A: 100000000 B: 100000000</span>
<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(</span><span class="s">"A: {}, B: {}"</span><span class="p">,</span>
<span class="w">       </span><span class="n">tensorA</span><span class="p">.</span><span class="n">data</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">tensorB</span><span class="p">.</span><span class="n">data</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div></td></tr></table></div>
</div>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
            <a href="community.html" title="Community Meetings, Channels &amp; Resources"
               class="md-flex md-footer-nav__link md-footer-nav__link--prev"
               rel="prev">
              <div class="md-flex__cell md-flex__cell--shrink">
                <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
              </div>
              <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
                <span class="md-flex__ellipsis">
                  <span
                      class="md-footer-nav__direction"> Previous </span> Community Meetings, Channels &amp; Resources </span>
              </div>
            </a>
          
          
            <a href="memory-management.html" title="Memory Management Principles"
               class="md-flex md-footer-nav__link md-footer-nav__link--next"
               rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span
                class="md-flex__ellipsis"> <span
                class="md-footer-nav__direction"> Next </span> Memory Management Principles </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink"><i
                class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2020, The Institute for Ethical AI &amp; Machine Learning.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 3.2.1.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>